{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MapReduce and PySpark  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few words about MapReduce\n",
    "MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure, which performs filtering and sorting, and a reduce method, which performs a summary operation. It has been implemented in several systems, including Google's internal implementation and the popular HDFS file system from Apache Foundation. It is suitable for handling huge datasets in distributed fashion. It only needs two functions *Map* and *Reduce* in order to work, while the system is responsible for the parallelism and the distribution of the jobs. For more concrete information read chapter 2 from the book (pages:23-35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will see some examples which are easy to be solved with MapReduce logic but also, we will see the procedure of a *Map* and *Reduce* functions in order to understand better the algorithms. Then, we will move to a MapReduce Framework called __[Spark](https://spark.apache.org/docs/0.9.0/index.html)__ . Apache Spark is a general purpuse cluster-computing system. It provides huge API's for many programming lunguages but we will deal only with the python part of spark called PySpark.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to understand the algorithms behind Map and Reduce, so for this purpose we will try to solve the word counting problem using only these two functions. The objective of this task is to implement a mapper and a reducer and solve the word counting problem in a way that the user will feed the system with a text and the output will be the unique words of the text and their frequencies next to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "System-Input  :  \"Hello Hello my name is George George George\"<br>\n",
    "System-Output : {\"Hello\" : 2 , \"my\" : 1 , \"name\" : 1 , \"is\" : 1 , \"George\" : 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the function \"mapper\" that splits the line into words and generates the key for each word.\n",
    "#### def mapper: \n",
    " - Input : text \n",
    " - Body  : split line into words, generate tuples in the form -> (word,1)\n",
    " - Output: return the generated tuples containing the key and the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    tups = [(w, 1) for w in words]\n",
    "    return tups    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function \"reducer\" that takes as input the mapper's output and groups by key (word) and sums up the frequency of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def reducer:\n",
    " - Input  : output of mapper\n",
    " - Body   : groupby key and sum up the frequency\n",
    " - Output : dictionary with as key the word and as value the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def reducer(tups):\n",
    "    d = defaultdict(int)\n",
    "    for w, c in tups:\n",
    "        d[w] += c\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test mapper's and reducer's functionality using a trivial string as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: line\n",
    "line = 'Advances in Data Mining Mining Data in Advances Data Mining in Advances in in Advances George'\n",
    "#\n",
    "# Call the mapper using the above string as input\n",
    "tups = mapper(line)\n",
    "#\n",
    "# Call reducer using the result of the mapper\n",
    "red_tups = reducer(tups)\n",
    "#\n",
    "# Print your results\n",
    "print(\"Mapper Output:\")\n",
    "list(map(print,tups))[0]\n",
    "\n",
    "print(\"\\nReducer Output:\")\n",
    "print(red_tups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Map'></a>\n",
    "#### Now instead of a signle string, we have a list of strings as input. <br>\n",
    "On the one hand we could call many times function 'mapper' inside a for loop which iterates the strings of the list but on the other-functional-hand, we can use python's build-in function called map and plug 'mapper' directly to our data. Function map breaks the input in small pieces and applies our function on each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input : List\n",
    "doc = ['Advances in Data Mining Mining Data in Advances Data Mining in Advances in in Advances',\n",
    "      'Science Fiction Science Movies Star Wars Star Wars',\n",
    "      'Advances Data Star Science Wars Advances Star Trek']\n",
    "# Call the 'mapper' function in a loop\n",
    "for i in doc:\n",
    "    print(mapper(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use function map\n",
    "# Basic systax map(function, data)\n",
    "l = list(map(mapper, doc))\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using functional tools we have the opportunity to avoid for loops. Moreover, functional tools are very helpful for cloud computing where documents exist in distributed file systems and we want to apply functions on each chunk of data in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the above function generated a list of lists one for each string. With the function below we can flatten those lists into one and then call the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "reducer(flatten(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other MapReduce Examples \n",
    "### Matrix Multiplication using MapReduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - #### For this example you need to define the function 'mapper_matmul' which takes as input two matrices in a sparse format and emits their indexes and their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us assume that we have two matrices of dim[5,5]  in the following sparse format: \n",
    "\n",
    "A,0,0,63 <br>\n",
    "A,0,1,45<br>\n",
    "A,0,2,93<br>\n",
    "A,0,3,32<br>\n",
    "A,0,4,49<br>\n",
    "    ... <br>\n",
    "    ...  \n",
    "B,0,0,63<br>\n",
    "B,0,1,18<br>\n",
    "B,0,2,89<br>\n",
    "B,0,3,28<br>\n",
    "B,0,4,39<br>\n",
    "...<br>\n",
    "...\n",
    "************\n",
    "##### This is our input file which contains 4 columns. \n",
    "1. The first column denotes the name of the matrix\n",
    "2. The second indicates the row\n",
    "3. The third indicates the column\n",
    "4. The fourth contains the value of the specific matrix\n",
    "\n",
    "##### It is essential to mention that sparse matrices A and B contain only the indeces of non zero values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the <font color=red>*input.txt*</font> file using the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrices = []\n",
    "with open('input.txt') as inputfile:\n",
    "    for line in inputfile:\n",
    "        line_ = line.strip()\n",
    "        matrices.append(line_)\n",
    "        \n",
    "matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the first function <font color=red>*mapper_matmul*</font>  \n",
    "#### def mapper_matmul: \n",
    " - Input : Two Sparse Matrices A & B (input.txt), number of rows of A and number of columns of B\n",
    " - Body  : For each line of the input emit indexing and values\n",
    " - Output: return the key, row/col, value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper_matmul(x,num_rows_A,num_cols_B):    \n",
    "    tups = []\n",
    "    for line in x:\n",
    "        matrix_index, row, col, value = line.rstrip().split(\",\")\n",
    "        if matrix_index == \"A\":\n",
    "            for i in range(0,num_cols_B):\n",
    "                key = row + \",\" + str(i)\n",
    "                tups.append(\"%s\\t%s\\t%s\"%(key,col,value))    \n",
    "                print (\"%s\\t%s\\t%s\"%(key,col,value))\n",
    "        else:\n",
    "            for j in range(0,num_rows_A):\n",
    "                key = str(j) + \",\" + col \n",
    "                tups.append(\"%s\\t%s\\t%s\"%(key,row,value))\n",
    "                print(\"%s\\t%s\\t%s\"%(key,row,value))\n",
    "    return tups         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the functionality of <font color=red>*mapper_matmul*</font>  using the object <font color=red>*matrices*</font> and see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_out = mapper_matmul(matrices,5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you have to define function <font color=red>*reducer_matmul*</font> which takes as input the <font color=red>*mapper_matmul*</font> output andapplies the dot product of the values. \n",
    "#### def reducer_matmul: \n",
    " - Input : Mapper Output (index, raw/col, value)\n",
    " - Body  : Dot product of the values\n",
    " - Output: return the index and the result of the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "def reducer_matmul(mapper_out):\n",
    "    prev_index = None\n",
    "    value_list = []\n",
    "    out = sorted(mapper_out)\n",
    "    tups = []\n",
    "    for line in out:\n",
    "        curr_index, index, value = line.strip().split(\"\\t\")\n",
    "        index, value = map(int,[index,value])\n",
    "        if curr_index == prev_index:\n",
    "            value_list.append((index,value))\n",
    "        else:\n",
    "            if prev_index:\n",
    "                value_list = sorted(value_list,key=itemgetter(0))\n",
    "                i = 0\n",
    "                result = 0\n",
    "                while i < len(value_list) - 1:\n",
    "                    if value_list[i][0] == value_list[i + 1][0]:\n",
    "                        result += value_list[i][1]*value_list[i + 1][1]\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        i += 1\n",
    "                print (\"%s,%s\"%(prev_index,str(result)))\n",
    "                tups.append(\"%s,%s\"%(prev_index,str(result)))\n",
    "            prev_index = curr_index\n",
    "            value_list = [(index,value)]\n",
    "\n",
    "    if curr_index == prev_index:\n",
    "        value_list = sorted(value_list,key=itemgetter(0))\n",
    "        i = 0\n",
    "        result = 0\n",
    "        while i < len(value_list) - 1:\n",
    "            if value_list[i][0] == value_list[i + 1][0]:\n",
    "                result += value_list[i][1]*value_list[i + 1][1]\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "    print (\"%s,%s\"%(prev_index,str(result)))\n",
    "    tups.append(\"%s,%s\"%(prev_index,str(result)))\n",
    "    return tups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test the reducer's result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matMul = reducer_matmul(map_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SettingSpark'></a>\n",
    "# Moving to PySpark\n",
    "0. #### Install pyspark using: ```conda install -c conda-forge pyspark```, probably you have to set-up PySpark environment in your .bashrc or .zshrc file. \n",
    "1. #### Load essential modules from pyspark. \n",
    "2. #### Set the application name to AiDM.\n",
    "3. #### Set the master_host to \"local\".\n",
    "4. #### Configure pyspark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf # 1\n",
    "Conf = SparkConf().setAppName('AiDM').setMaster(\"local\") # Steps 2 & 3\n",
    "sc = SparkContext(conf=Conf) # 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check pySpark's configuration and click the link below to visit PySpark's UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a big dataset with text and implement word counting with MapReduce framwork in <font color=red>__[PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "with open('example_text.txt') as inputfile:\n",
    "    for line in inputfile:\n",
    "        line_ = line.strip().split(',')\n",
    "        if line_ != ['']:\n",
    "            results.append(line_[0])\n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function for testing the format of results in order to use PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CHECK(results):\n",
    "    if (type(results) == list) and (type(results[0]) == str):\n",
    "        print(\">_ Bingoo! you can proceed  >_\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Check the structure of your data and try again...\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if object **'results'** is in a good format\n",
    "We need a list which contains strings. \n",
    "So each line of of the text has to be a string in the list.\n",
    "Use the function <font color=red>CHECK</font> for this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHECK(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you can parallelize your data using the PySpark function <font color=red>*parallelize*</font> which parallelizes your data using the <font color=red>*sc*</font>  session from [Section: Moving to Spark](#SettingSpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function <font color=red>*sc.parallelize*</font> understands the structure of the data and organizes their distribution for further processing. It generates a __[ParallelCollectionRDD](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD)__  object, which is managable from PySpark framework. It can either handle data which are loaded to RAM or data from a DFS storage space (cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_rdd = sc.parallelize(results)\n",
    "docs_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous chunk [map()](#Map), map the function <font color=red>*mapper*</font> into docs_rdd and return the output using spark function <font color=red>*collect()*</font>. Function <font color=red>*flatMap*</font> returns an RDD object with flatten output as list of elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tups_rdd = docs_rdd.flatMap(mapper)\n",
    "tups_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use function <font color=red>*reduceByKey()*</font> which first groupes the RDD object by key (key=word in this example) and then reduce the output by applying accumulation on the item frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tups_rdd.reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counting in Functional-MapReduce fashion\n",
    "#### All the above procedure can be written in only three lines using PySpark's API and MapReduce logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Parallelize input data \n",
    "text_file = sc.parallelize(results)\n",
    "#\n",
    "# 1:   Map the lambda-function which splits the lines and flatten them (.flatMap)\n",
    "# 2:   Then emit each word and the number one for each word (.map)\n",
    "# 3:   Then groupByKey (word) and sum-up the ones that mapper emits (.reduceByKey)\n",
    "#\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)                               \n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF PySpark's implementation \n",
    "Term Frequency–Inverse Document Frequency is a numerical statistic measure which reflects the importance of each word in a document inside a document-corpus. It is basicaly used in information retrieval for ranking documents while running queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF(t) = (Number of times term 't' appears in a document) / (Total number of terms in the document)<br>\n",
    "IDF(t) = log(Total number of documents / Number of documents with term t in it)\n",
    "\\begin{equation*}\n",
    "TFIDF =TF \\times IDF \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure of TF-IDF computation in this example is made by using the Hashing-trick. In this way, each word takes a unique random number for id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "# $example off$\n",
    "\n",
    "doc_1 = ['Advances Advances Advances Computer Computer Science Science George',\n",
    "         'Advances Science Science Science George',\n",
    "        'Advances George Science Science',\n",
    "        'Science Science George']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doc = sc.parallelize(doc_1)\n",
    "    # $example on$\n",
    "    # Load documents (one per line).\n",
    "    documents = doc.map(lambda line: line.split(\" \"))\n",
    "\n",
    "    hashingTF = HashingTF()\n",
    "    tf = hashingTF.transform(documents)\n",
    "\n",
    "    # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "    # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "    tf.cache()\n",
    "    idf = IDF().fit(tf)\n",
    "    tfidf = idf.transform(tf)\n",
    "\n",
    "    # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "    # which occur in less than a minimum number of documents.\n",
    "    # In such cases, the IDF for these terms is set to 0.\n",
    "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "    idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "    tfidfIgnore = idfIgnore.transform(tf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   tfidf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication in Spark (pyspark)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{A} \\times \\mathbf{B} =  \\mathbf{P}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{vmatrix}\n",
    "\\mathbf{a}_{00} & \\mathbf{a}_{01} & \\mathbf{a}_{02}\\\\\n",
    "\\mathbf{a}_{10} & \\mathbf{a}_{11} & \\mathbf{a}_{12}\\\\\n",
    "\\end{vmatrix} \\\n",
    "\\\n",
    "\\times\n",
    "\\\n",
    "\\begin{vmatrix}\n",
    "\\mathbf{b}_{00} & \\mathbf{b}_{01} \\\\\n",
    "\\mathbf{b}_{10} & \\mathbf{b}_{11} \\\\\n",
    "\\mathbf{b}_{20} & \\mathbf{b}_{21} \\\\\n",
    "\\end{vmatrix} \n",
    "\\\n",
    "=\n",
    "\\\n",
    "\\begin{vmatrix}\n",
    " \\mathbf{a}_{00} \\times \\mathbf{b}_{00} + \\\n",
    " \\mathbf{a}_{01} \\times \\mathbf{b}_{10} + \\\n",
    " \\mathbf{a}_{02} \\times \\mathbf{b}_{20}  &\\\n",
    " \\\n",
    " \\mathbf{a}_{00} \\times \\mathbf{b}_{01} +\\\n",
    " \\mathbf{a}_{01} \\times \\mathbf{b}_{11} +\\\n",
    " \\mathbf{a}_{02} \\times \\mathbf{b}_{21} &\\\n",
    " \\\\\n",
    " \\mathbf{a}_{10} \\times \\mathbf{b}_{00} + \\\n",
    " \\mathbf{a}_{11} \\times \\mathbf{b}_{10}  +\\\n",
    " \\mathbf{a}_{12} \\times \\mathbf{b}_{20}  &\\\n",
    " \\\n",
    " \\mathbf{a}_{10} \\times \\mathbf{b}_{01} + \\\n",
    " \\mathbf{a}_{11} \\times \\mathbf{b}_{11}  +\\\n",
    " \\mathbf{a}_{12} \\times \\mathbf{b}_{21}  &\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate random matrices <font color=red>A</font> and <font color=red>B</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "L=2\n",
    "M=3\n",
    "N=2\n",
    "a_shape = (L,M)\n",
    "b_shape = (M,N)\n",
    "p_shape = (a_shape[0], b_shape[1])\n",
    "\n",
    "\n",
    "a = np.random.randint(100, size = a_shape)\n",
    "b = np.random.randint(100, size = b_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make them in sparse format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparsify(a):\n",
    "    rv = []\n",
    "    for i, line in enumerate(a):\n",
    "        for (j, elem) in enumerate(line):\n",
    "            if elem!=0: rv.append(((i,j),elem))\n",
    "    return rv\n",
    "        \n",
    "sparse_a = sparsify(a)\n",
    "sparse_b = sparsify(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def desparsify(sparse, shape):\n",
    "    a = np.zeros(shape)\n",
    "    for (i, j), e in sparse:\n",
    "        a[i][j] = e\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the transformation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array_equal(a, desparsify(sparse_a, a_shape)) and \\\n",
    "np.array_equal(b, desparsify(sparse_b, b_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make two mappers one for each matrix.<br>\n",
    "Mappers have to produce:<br>\n",
    "For each element i,j in A, emit([i,k], A[i,j]) for k=1...N, where N: Number of columns of matrix B<br>\n",
    "For each element j,k in B, emit([i,k], B[j,k]) for i=1...L, where L: Number of rows of matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First construct and RDD object\n",
    "a_rdd = sc.parallelize(sparse_a)\n",
    "\n",
    "def a_mapper(elem, N):\n",
    "    (i, j), Aij = elem\n",
    "    acc = []\n",
    "    for k in range(N):\n",
    "        acc.append(((i,k),(j, Aij)))\n",
    "    return acc\n",
    "# Map a_mapper to a_rdd object\n",
    "a_interm = a_rdd.flatMap(lambda e: a_mapper(e, b_shape[1]))\n",
    "a_interm.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_rdd = sc.parallelize(sparse_b)\n",
    "\n",
    "def b_mapper(elem, L):\n",
    "    (j, k), Bjk = elem\n",
    "    acc = []\n",
    "    for i in range(L):\n",
    "        acc.append(((i,k),(j, Bjk)))\n",
    "    return acc\n",
    "b_interm = b_rdd.flatMap(lambda e: b_mapper(e, b_shape[1]))\n",
    "b_interm.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the two outputs in one by taking the union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_interm = a_interm.union(b_interm)\n",
    "all_interm.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function reducer which applies the dot product of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer(tup):\n",
    "    idx, vals = tup\n",
    "    acc = {}\n",
    "    for i,v in vals:\n",
    "        if i in acc:\n",
    "            acc[i] *= v\n",
    "        else:\n",
    "            acc[i] = v\n",
    "            \n",
    "    return (idx, sum(acc.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use PySpark's functional API and plug the functions into data\n",
    "1. GroupByKey the list 'all_interm' RDD object\n",
    "2. Then map the reducer on it\n",
    "3. Collect the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = all_interm \\\n",
    "    .groupByKey() \\\n",
    "    .map(reducer) \\\n",
    "    .collect()\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if matrix multiplication is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links \n",
    "__[Map-Reduce Examples](http://www.science.smith.edu/dftwiki/index.php/Map-Reduce_Examples)__ <br>\n",
    "__[Hadoop-MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm)__ <br>\n",
    "__[Python Functional tools](http://book.pythontips.com/en/latest/map_filter.html)__ <br>\n",
    "__[Python Examples](https://medium.com/@nidhog/how-to-quickly-write-a-mapreduce-framework-in-python-821a79fda554)__ <br>\n",
    "__[Lambda](https://www.python-course.eu/lambda.php)__\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
